#!/usr/bin/env python3
"""
Process Obsidian Daily Digests from Batch Read

Processes multiple daily digest markdown contents and generates
task-entry-*.json files for analytics.

Usage:
    python scripts/process-obsidian-batch.py
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Any


STATE_DIR = Path("cortex/state")


def extract_timerange_from_text(text: str) -> tuple[str | None, str | None, str]:
    """
    Extract time range from text like (18:20-18:30 JST).
    Returns (start_time, end_time, cleaned_text) tuple.
    Times are returned in HH:MM format.
    """
    # Pattern: (HH:MM-HH:MM JST) or (HH:MM-HH:MM)
    timerange_pattern = r'\((\d{2}:\d{2})-(\d{2}:\d{2})(?:\s*JST)?\)'
    match = re.search(timerange_pattern, text)

    if match:
        start_time = match.group(1)
        end_time = match.group(2)
        # Remove the time range from text
        text = re.sub(timerange_pattern, '', text).strip()
        text = re.sub(r'\s+', ' ', text).strip()
        return start_time, end_time, text

    return None, None, text


def extract_duration_from_text(text: str) -> tuple[int | None, str, str]:
    """
    Extract duration from text using multiple patterns.
    Returns (duration_minutes, duration_source, cleaned_text) tuple.

    duration_source values:
    - "explicit": Explicitly stated duration (10åˆ†, 10m, **æ‰€è¦æ™‚é–“**: 10m)
    - None: No duration found
    """
    duration = None
    duration_source = None

    # Try multiple patterns
    duration_patterns = [
        (r'\((\d+)åˆ†\)', 1),         # (30åˆ†)
        (r'(\d+)åˆ†', 1),             # 30åˆ†
        (r'\((\d+)m\)', 1),          # (10m)
        (r'(\d+)m(?!\w)', 1),        # 10m (not followed by word chars)
        (r'\*\*æ‰€è¦æ™‚é–“\*\*:\s*(\d+)m', 1),  # **æ‰€è¦æ™‚é–“**: 10m
        (r'\*\*æ‰€è¦æ™‚é–“\*\*:\s*(\d+)åˆ†', 1),  # **æ‰€è¦æ™‚é–“**: 10åˆ†
    ]

    for pattern, _ in duration_patterns:
        duration_match = re.search(pattern, text)
        if duration_match:
            duration = int(duration_match.group(1))
            duration_source = "explicit"
            # Remove the duration from text
            text = re.sub(pattern, '', text).strip()
            # Clean up extra whitespace
            text = re.sub(r'\s+', ' ', text).strip()
            break

    return duration, duration_source, text


def parse_daily_digest(content: str, date_str: str) -> Dict[str, Any]:
    """
    Parse a daily digest markdown content and extract task data.

    Supports multiple formats:
    1. Task list: - [x] Task title (30åˆ†)
    2. Table: | Time | Task | Duration |
    3. Progress section: ### Title (time) + **æ‰€è¦æ™‚é–“**: Xm

    Returns task entry dict in format:
    {
        "tasks": [
            {
                "title": "Task title",
                "status": "completed" | "incomplete",
                "category": "high-priority" | "normal" | "untagged",
                "completed_at": "ISO8601" or None,
                "duration_minutes": int or None
            }
        ]
    }
    """
    tasks = []
    current_category = "untagged"

    lines = content.split("\n")

    # Phase 1: Parse task list format
    for line in lines:
        line = line.strip()

        # Detect category headers
        if "å„ªå…ˆåº¦ï¼šé«˜" in line or "å„ªå…ˆåº¦: é«˜" in line or "High Priority" in line:
            current_category = "high-priority"
            continue
        elif "é€šå¸¸ã‚¿ã‚¹ã‚¯" in line or "Regular Tasks" in line:
            current_category = "normal"
            continue
        elif "ã‚¿ã‚°ãªã—ã‚¿ã‚¹ã‚¯" in line or "No Tags" in line:
            current_category = "untagged"
            continue

        # Parse task lines: - [x] or - [ ]
        task_match = re.match(r'^-\s*\[([ xXâœ“])\]\s+(.+)$', line)
        if task_match:
            is_completed = task_match.group(1).lower() in ['x', 'âœ“']
            task_text = task_match.group(2).strip()

            # Skip placeholder tasks
            if "ã‚¿ã‚¹ã‚¯ãªã—" in task_text or "ä»Šæ—¥ã®ä¸»ãªé€²æ—" in task_text:
                continue

            # Extract time range first (highest priority)
            start_time, end_time, task_text = extract_timerange_from_text(task_text)

            # Extract duration using unified function
            duration, duration_source, task_text = extract_duration_from_text(task_text)

            task = {
                "title": task_text,
                "status": "completed" if is_completed else "incomplete",
                "category": current_category
            }

            # Timestamp handling with source/confidence
            if start_time and end_time:
                # Timerange found: highest confidence
                task["started_at"] = f"{date_str}T{start_time}:00+09:00"
                task["completed_at"] = f"{date_str}T{end_time}:00+09:00"
                task["timestamp_source"] = "timerange"
                task["timestamp_confidence"] = 0.7

                # Calculate duration from timerange if not explicitly provided
                if not duration:
                    start_h, start_m = map(int, start_time.split(':'))
                    end_h, end_m = map(int, end_time.split(':'))
                    duration_mins = (end_h * 60 + end_m) - (start_h * 60 + start_m)
                    if 1 <= duration_mins <= 240:  # Sanity check: 1-240 minutes
                        task["duration_minutes"] = duration_mins
                        task["duration_source"] = "timerange"
                        task["duration_confidence"] = 0.7
            elif is_completed:
                # No timerange: use fixed placeholder (æ–¹é‡B)
                task["completed_at"] = f"{date_str}T01:00:00Z"
                task["timestamp_source"] = "fixed"
                task["timestamp_confidence"] = 0.1
            else:
                task["completed_at"] = None
                task["timestamp_source"] = "unknown"
                task["timestamp_confidence"] = 0.0

            # Duration handling with source/confidence
            if duration and duration_source == "explicit":
                task["duration_minutes"] = duration
                task["duration_source"] = "explicit"
                task["duration_confidence"] = 1.0
            elif "duration_source" not in task:
                # No duration info at all
                task["duration_source"] = "unknown"
                task["duration_confidence"] = 0.0

            tasks.append(task)

    # Phase 2: Parse table format (| Time | Task | Duration |)
    in_table = False
    for i, line in enumerate(lines):
        line = line.strip()

        # Detect table header row
        if re.match(r'\|\s*æ™‚åˆ»\s*\|.*\|\s*æ™‚é–“\s*\|', line, re.IGNORECASE):
            in_table = True
            continue

        # Skip separator row
        if in_table and re.match(r'\|[-:\s]+\|', line):
            continue

        # Parse table data rows
        if in_table and line.startswith('|') and not re.match(r'\|[-:\s]+\|', line):
            parts = [p.strip() for p in line.split('|')]
            if len(parts) >= 4:  # | time | task | duration |
                time_text = parts[1]
                task_title = parts[2]
                duration_text = parts[3] if len(parts) > 3 else ""

                # Extract time range from time column
                start_time, end_time, _ = extract_timerange_from_text(time_text)

                # Extract duration from the duration column
                duration, duration_source, _ = extract_duration_from_text(duration_text)

                if task_title and task_title != "ã‚¿ã‚¹ã‚¯":
                    task = {
                        "title": task_title,
                        "status": "completed",  # Table tasks are assumed completed
                        "category": "normal"
                    }

                    # Timestamp handling
                    if start_time and end_time:
                        task["started_at"] = f"{date_str}T{start_time}:00+09:00"
                        task["completed_at"] = f"{date_str}T{end_time}:00+09:00"
                        task["timestamp_source"] = "timerange"
                        task["timestamp_confidence"] = 0.7

                        # Calculate duration from timerange if not in duration column
                        if not duration:
                            start_h, start_m = map(int, start_time.split(':'))
                            end_h, end_m = map(int, end_time.split(':'))
                            duration_mins = (end_h * 60 + end_m) - (start_h * 60 + start_m)
                            if 1 <= duration_mins <= 240:
                                task["duration_minutes"] = duration_mins
                                task["duration_source"] = "timerange"
                                task["duration_confidence"] = 0.7
                    else:
                        # No time info: fixed placeholder
                        task["completed_at"] = f"{date_str}T01:00:00Z"
                        task["timestamp_source"] = "fixed"
                        task["timestamp_confidence"] = 0.1

                    # Duration handling
                    if duration and duration_source == "explicit":
                        task["duration_minutes"] = duration
                        task["duration_source"] = "explicit"
                        task["duration_confidence"] = 1.0
                    elif "duration_source" not in task:
                        task["duration_source"] = "unknown"
                        task["duration_confidence"] = 0.0

                    tasks.append(task)
        elif in_table and not line.startswith('|'):
            # End of table
            in_table = False

    # Phase 3: Parse progress section format (### Title (HH:MM-HH:MM JST) + **æ‰€è¦æ™‚é–“**: Xm)
    for i, line in enumerate(lines):
        line = line.strip()

        # Detect section header with time range: ### Title (HH:MM-HH:MM JST)
        section_match = re.match(r'^###\s+(.+?)\s*\((\d{2}:\d{2})-(\d{2}:\d{2})\s*JST\)', line)
        if section_match:
            section_title = section_match.group(1).strip()
            start_time = section_match.group(2)
            end_time = section_match.group(3)

            # Look ahead for **æ‰€è¦æ™‚é–“**: Xm in the next few lines
            duration = None
            duration_source = None
            for j in range(i + 1, min(i + 5, len(lines))):
                next_line = lines[j].strip()
                # Check for duration marker
                if '**æ‰€è¦æ™‚é–“**:' in next_line or '**ã‚«ãƒ†ã‚´ãƒª**:' in next_line:
                    dur, dur_src, _ = extract_duration_from_text(next_line)
                    if dur:
                        duration = dur
                        duration_source = dur_src
                        break
                # Stop if we hit another section or empty lines
                if next_line.startswith('###') or (not next_line and j > i + 2):
                    break

            task = {
                "title": section_title,
                "status": "completed",
                "category": "normal",
                "started_at": f"{date_str}T{start_time}:00+09:00",
                "completed_at": f"{date_str}T{end_time}:00+09:00",
                "timestamp_source": "timerange",
                "timestamp_confidence": 0.7
            }

            # Duration handling
            if duration and duration_source == "explicit":
                task["duration_minutes"] = duration
                task["duration_source"] = "explicit"
                task["duration_confidence"] = 1.0
            else:
                # Calculate from timerange
                start_h, start_m = map(int, start_time.split(':'))
                end_h, end_m = map(int, end_time.split(':'))
                duration_mins = (end_h * 60 + end_m) - (start_h * 60 + start_m)
                if 1 <= duration_mins <= 240:
                    task["duration_minutes"] = duration_mins
                    task["duration_source"] = "timerange"
                    task["duration_confidence"] = 0.7
                else:
                    task["duration_source"] = "unknown"
                    task["duration_confidence"] = 0.0

            tasks.append(task)

    return {"tasks": tasks}


# Digest data from MCP batch read
DIGEST_DATA = {
    "2025-11-17": """# Daily Digest - 2025-11-17

## Tasks
- [x] Recipe 1 å®Œäº†ï¼ˆObsidian â†’ Slack é€šçŸ¥ï¼‰
- [x] Recipe 2 å®Œäº†ï¼ˆå®šæœŸ KB å†æ§‹ç¯‰ï¼‰
- [x] Recipe 3 å®Œæˆï¼ˆDaily Digest to Slackï¼‰
- [x] Phase 2 åŸºç›¤æ§‹ç¯‰å®Œäº†

## Reflection
- Phase 2 ã®åŸºç›¤ãŒæ•´ã£ãŸ
- Recipe 1-3 ãŒå…¨ã¦å‹•ä½œç¢ºèªæ¸ˆã¿
- è‡ªå‹•åŒ–ã®åœŸå°ãŒã§ããŸ""",

    "2025-11-18": """# Daily Digest - 2025-11-18

## Tasks
- [x] Recipe 3 å®Œæˆï¼ˆDaily Digest to Slackï¼‰
- [x] Phase 2 åŸºç›¤æ§‹ç¯‰å®Œäº†
- [x] Recipe 9 ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- [x] Weekly Summary ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è¨­è¨ˆ
- [x] Claude Code é€£æºå‹•ä½œç¢ºèª

## Reflection
- Phase 2 ã®è‡ªå‹•åŒ–åŸºç›¤ãŒæ•´ã£ãŸ
- n8n ã¨ Obsidian ã®é€£æºãŒã‚¹ãƒ ãƒ¼ã‚ºã«å‹•ä½œ
- KB å¤œé–“è‡ªå‹•å†æ§‹ç¯‰ã€æœã® Digest é…ä¿¡ã€é‡è¦ãƒãƒ¼ãƒˆå¤‰æ›´é€šçŸ¥ã®ãƒ«ãƒ¼ãƒ—ãŒå®Œæˆ
- æ¬¡ã¯ Claude Code ã¨ã®ç›´çµï¼ˆRecipe 9ï¼‰ã§é–‹ç™ºä½“é¨“ã‚’ã•ã‚‰ã«å‘ä¸Š""",

    "2025-11-19": """# Daily Digest 2025-11-19

## Tasks
- [ ] Cortex OS Weekly ãƒ†ã‚¹ãƒˆ
- [ ] Recipe 11 å‹•ä½œç¢ºèª
- [ ] Continue On Fail è¨­å®šè¿½åŠ 

## Reflection
- Weekly Summary ã®ãƒ†ã‚¹ãƒˆç”¨ã«ä½œæˆ
- Phase 2.1 è‡ªå‹•åŒ–åŸºç›¤ã®æ¤œè¨¼ä¸­""",

    "2025-11-25": """### âœ… å®Œäº†ã‚¿ã‚¹ã‚¯ï¼ˆæ™‚ç³»åˆ—ï¼‰

#### 1. ç’°å¢ƒæº–å‚™ãƒ»ç¢ºèª
- [x] /init ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹å¾©å…ƒ
- [x] Recipe 13 å®Ÿè¡Œçµæœç¢ºèªï¼ˆ22:00è‡ªå‹•å®Ÿè¡ŒæˆåŠŸï¼‰
- [x] tomorrow.json æ¤œè¨¼ï¼ˆæ­£å¸¸ç”Ÿæˆç¢ºèªï¼‰
- [x] Phase 2 å®Ÿè£…çŠ¶æ³ç¢ºèª

#### 2. n8n ãƒ‡ãƒ—ãƒ­ã‚¤æº–å‚™
- [x] ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç¢ºèª
- [x] ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ é¸æŠï¼ˆRailway æ±ºå®šï¼‰
- [x] .env.production ä½œæˆ
- [x] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚­ãƒ¼ç”Ÿæˆ
- [x] .gitignore ç¢ºèªï¼ˆæ©Ÿå¯†æƒ…å ±ä¿è­·ï¼‰

#### 3. Railway è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
- [x] railway.json ä½œæˆï¼ˆãƒ“ãƒ«ãƒ‰è¨­å®šï¼‰
- [x] Dockerfile.railway ä½œæˆï¼ˆRailwayæœ€é©åŒ–ï¼‰
- [x] README-RAILWAY.md ä½œæˆï¼ˆãƒ‡ãƒ—ãƒ­ã‚¤ã‚¬ã‚¤ãƒ‰ï¼‰
- [x] Caddyfile ä½œæˆï¼ˆè‡ªå‰VPSç”¨ï¼‰
- [x] deploy-production.sh ä½œæˆï¼ˆè‡ªå‰VPSç”¨ï¼‰""",

    "2025-11-26": """#### 1. å•é¡Œèª¿æŸ»
- [x] Obsidian MCP ã‚¨ãƒ©ãƒ¼ç¢ºèªï¼ˆ2ä»¶ï¼‰
- [x] ç’°å¢ƒå¤‰æ•°ç¢ºèªï¼ˆMCP_OBSIDIAN_* ã™ã¹ã¦æ­£å¸¸ï¼‰
- [x] Obsidian ãƒ—ãƒ­ã‚»ã‚¹ç¢ºèªï¼ˆæœªèµ·å‹•ã‚’ç¢ºèªï¼‰

#### 2. æ ¹æœ¬åŸå› ç‰¹å®š
- [x] PORT 27124 æ¥ç¶šãƒ†ã‚¹ãƒˆï¼ˆå¤±æ•—ï¼‰
- [x] lsof ã§ãƒãƒ¼ãƒˆç¢ºèªï¼ˆãƒªã‚¹ãƒ‹ãƒ³ã‚°ãªã—ï¼‰
- [x] curl ãƒ†ã‚¹ãƒˆï¼ˆConnection refusedï¼‰
- [x] **æ ¹æœ¬åŸå› ç¢ºå®š**: Obsidian REST API æœªèµ·å‹•""",

    "2025-11-27": """**å®Ÿè£…å®Œäº†**:
- âœ… build-embeddings.mjs å®Ÿè£…ãƒ»å®Ÿè¡Œ (184 concepts â†’ embeddings)
- âœ… cluster.mjs å®Ÿè£…ãƒ»å®Ÿè¡Œ (Connected Components, threshold: 0.7)
- âœ… export-graph.mjs å®Ÿè£…ãƒ»å®Ÿè¡Œ (graph-v1.json + clusters-v1.md)""",

    "2025-11-28": """### High Priority
  - [ ] cortex/kb/embed.mjs refactoring #urgent #deepwork
  - [ ] Production deployment blocked by infra #blocked
  - [ ] Waiting for design team feedback #waiting

  ### Regular Tasks
  - [ ] cortex/graph/export.mjs ãƒ†ã‚¹ãƒˆè¿½åŠ  #review
  - [ ] n8n Recipe 03 documentation #review
  - [ ] Research new caching strategy #deepwork

  ### No Tags
  - [ ] Update README
  - [ ] Code cleanup""",

    "2025-12-01": """### ğŸ‰ Major Achievement
     KB Rebuildå®Œå…¨å®Ÿè£…ï¼11æ™‚é–“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§:
     - Recipe 02å®Œæˆï¼ˆn8n workflowï¼‰
     - Hash-based embeddingsï¼ˆ146 files â†’ 732 chunksï¼‰
     - èªè¨¼è¨­å®šãƒ»ã‚¨ãƒ©ãƒ¼ä¿®æ­£
     - Health Score 90%é”æˆ""",

    "2025-12-06": """### å„ªå…ˆåº¦ï¼šé«˜
1. â° Recipe 03/10 å®Ÿè¡Œãƒ­ã‚°ç¢ºèªï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼ç™ºè¦šï¼‰

### Regular Tasks
| æ™‚åˆ»  | ã‚¿ã‚¹ã‚¯                     | æ™‚é–“  |
|-------|---------------------------|-------|
| 17:00+ | Recipe 03/10 å®Ÿè¡Œãƒ­ã‚°ç¢ºèª   | 10åˆ†  |
| 17:15+ | Health Score åˆå›è¨ºæ–­      | 15åˆ†  |
| 17:35+ | /suggest v2.0 å‹•ä½œç¢ºèª     | 20åˆ†  |""",

    "2025-12-08": """### âœ… å®Œäº†ã‚¿ã‚¹ã‚¯ï¼ˆæ™‚ç³»åˆ—ï¼‰

#### 1. ç’°å¢ƒæº–å‚™ãƒ»ç¢ºèª
- [x] /init ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹å¾©å…ƒ
- [x] Recipe 13 å®Ÿè¡Œçµæœç¢ºèªï¼ˆ22:00è‡ªå‹•å®Ÿè¡ŒæˆåŠŸï¼‰
- [x] tomorrow.json æ¤œè¨¼ï¼ˆæ­£å¸¸ç”Ÿæˆç¢ºèªï¼‰
- [x] Phase 2 å®Ÿè£…çŠ¶æ³ç¢ºèª

#### 2. n8n ãƒ‡ãƒ—ãƒ­ã‚¤æº–å‚™
- [x] ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç¢ºèª
- [x] ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ é¸æŠï¼ˆRailway æ±ºå®šï¼‰""",

    "2025-12-09": """### å„ªå…ˆåº¦ï¼šé«˜
- [x] æ—¥æ¬¡ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆç”Ÿæˆãƒã‚°ä¿®æ­£
- [x] Recipe 03ä¿®å¾©
- [x] GitHub pushã¾ã§å®Œäº†
- [x] ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­å®Ÿè¡Œ""",

    "2025-12-10": """### å„ªå…ˆåº¦ï¼šé«˜
- [x] Recipe ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ä¿®æ­£å®Œäº† (10åˆ†)
- [x] Recipe 10 è¨ºæ–­ã¨èª¤è¨ºã®è¨‚æ­£ (35åˆ†)
- [x] èª¤è¨ºç”¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‰Šé™¤
- [x] Recipe 10 å†æœ‰åŠ¹åŒ–""",

    "2025-12-12": """### å„ªå…ˆåº¦ï¼šé«˜
- [x] Recipe å‹•ä½œç¢ºèª
- [x] Recipe 14 æ—¥ä»˜ãƒ­ã‚¸ãƒƒã‚¯ä¿®æ­£
- [x] Recipe 02 kb-api å‚ç…§å‰Šé™¤
- [x] ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å†è¨­å®šï¼ˆ22:10ã‚¹ã‚¿ãƒ¼ãƒˆï¼‰""",

    "2025-12-13": """### å„ªå…ˆåº¦ï¼šé«˜
- [x] v1.2 Roadmap ç¢ºèª
- [x] Recipe å‹•ä½œç¢ºèª
- [x] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´ç†""",

    "2025-12-14": """### å„ªå…ˆåº¦ï¼šé«˜
- [ ] v1.2 Roadmap ç¢ºèª
- [ ] Recipe å‹•ä½œç¢ºèª
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´ç†""",

    "2025-12-19": """### å„ªå…ˆåº¦ï¼šé«˜
- [x] v1.2 Roadmap ç¢ºèª
- [x] Recipe å‹•ä½œç¢ºèª
- [x] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´ç†""",
}


def load_digest_files() -> Dict[str, str]:
    """Load digest files from cortex/daily/ directory."""
    daily_dir = Path("cortex/daily")
    if not daily_dir.exists():
        return {}

    digest_data = {}
    for file_path in sorted(daily_dir.glob("????-??-??-digest.md")):
        # Extract date from filename: 2025-12-22-digest.md -> 2025-12-22
        date_str = file_path.stem.replace("-digest", "")

        try:
            with file_path.open("r", encoding="utf-8") as f:
                content = f.read()
                digest_data[date_str] = content
        except Exception as e:
            print(f"âš ï¸  Error reading {file_path}: {e}", file=sys.stderr)

    return digest_data


def main():
    import argparse
    parser = argparse.ArgumentParser(description='Process Obsidian daily digests')
    parser.add_argument('--from-files', action='store_true',
                       help='Load digests from cortex/daily/*.md files instead of DIGEST_DATA')
    args = parser.parse_args()

    STATE_DIR.mkdir(parents=True, exist_ok=True)

    # Choose data source
    if args.from_files:
        digest_data = load_digest_files()
        source = "cortex/daily/"
    else:
        digest_data = DIGEST_DATA
        source = "DIGEST_DATA (embedded)"

    processed = 0
    extracted_total = 0
    duration_count = 0

    print(f"ğŸ” Processing {len(digest_data)} daily digests from {source}...")
    print(f"   Output: {STATE_DIR}")
    print()

    for date_str, content in sorted(digest_data.items()):
        # Parse digest
        task_data = parse_daily_digest(content, date_str)
        task_count = len(task_data["tasks"])

        if task_count == 0:
            print(f"â­ï¸  {date_str}: No tasks found, skipping")
            continue

        # Count tasks with duration
        tasks_with_duration = sum(1 for t in task_data["tasks"] if "duration_minutes" in t)

        # Write task entry JSON
        output_file = STATE_DIR / f"task-entry-{date_str}.json"
        with output_file.open("w", encoding="utf-8") as f:
            json.dump(task_data, f, ensure_ascii=False, indent=2)

        duration_info = f" ({tasks_with_duration} with duration)" if tasks_with_duration > 0 else ""
        print(f"âœ… {date_str}: {task_count} tasks{duration_info} â†’ {output_file}")
        processed += 1
        extracted_total += task_count
        duration_count += tasks_with_duration

    print()
    print(f"ğŸ“Š Summary:")
    print(f"   Files processed: {processed}")
    print(f"   Tasks extracted: {extracted_total}")
    print(f"   Tasks with duration: {duration_count}")
    print()
    print("âœ… Task extraction complete!")
    print()
    print("Next steps:")
    print("   1. Run analytics: python3 scripts/analyze-duration.py")
    print("   2. Run analytics: python3 scripts/analyze-category-heatmap.py")
    print("   3. Run analytics: python3 scripts/analyze-rhythm.py")
    print("   4. Run health check: python3 scripts/analyze-health.py --window-days 7")


if __name__ == "__main__":
    import sys
    main()
